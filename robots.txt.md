
Date: 2025-09-28
Tags: 
Links: 

***

`robots.txt`とは、Webサイトの運営者が、検索エンジンのクローラー（ロボット）などのプログラムに対して、サイト内のどのページをクロール（巡回・情報収集）してよいか、あるいはしないでほしいかを伝えるための指示書のようなテキストファイルです。

### 主な目的
1.  **クロールの制御:** サーバーに過度な負荷がかかるのを防いだり、サイト内の重要でないページ（例: 管理者ページ、検索結果ページ、重複コンテンツなど）をクローラーに巡回させないようにします。
2.  **クロール効率の向上:** サイトマップ（`Sitemap`）の場所を伝えることで、クローラーにサイトの構造を効率的に理解させ、重要なページを優先的にクロールしてもらう手助けをします。

### 仕組み
1.  クローラーはWebサイトにアクセスする際、まず最初にそのサイトのルートディレクトリにある `robots.txt` ファイルを探します。（例: `https://www.example.com/robots.txt`）
2.  ファイルがあれば、その中の指示を読み取ります。
3.  その指示に従って、許可されたページのみをクロールします。

### 基本的な書き方
`robots.txt`は、主に以下の2つの命令で構成されます。

-   `User-agent`: 対象となるクローラーを指定します。`*` を指定すると、すべてのクローラーが対象になります。
-   `Disallow`: クロールを禁止したいディレクトリやファイルを指定します。
-   `Allow`: `Disallow`で禁止したディレクトリ内の一部を、例外的にクロール許可したい場合に指定します。
-   `Sitemap`: サイトマップファイルのURLを指定し、クローラーにサイトの全ページを知らせます。

#### 書き方の例
```txt
# すべてのクローラーが対象
User-agent: *

# /admin/ ディレクトリ以下のクロールを禁止
Disallow: /admin/

# /private/ ディレクトリ以下のクロールを禁止
Disallow: /private/

# ただし、/private/public.html だけはクロールを許可
Allow: /private/public.html

# Googleのクローラーだけが対象
User-agent: Googlebot

# /for-google-only/ ディレクトリはGoogleにだけクロールを許可
Allow: /for-google-only/

# サイトマップの場所を伝える
Sitemap: https://www.example.com/sitemap.xml
```

### 重要な注意点
-   **強制力はない:** `robots.txt`の指示はあくまで「お願い」です。礼儀正しい主要なクローラー（Googlebotなど）は従いますが、悪意のあるボットや一部のクローラーは無視することがあります。
-   **セキュリティ機能ではない:** このファイルは誰でも見ることができます。そのため、機密情報や非公開にしたいページのURLを`Disallow`に書いても、隠すことにはなりません。むしろ、そのURLの存在を知らせてしまうことになりかねません。
-   **インデックス登録の禁止ではない:** `Disallow`に指定しても、他のサイトからリンクが張られている場合など、そのページが検索結果に表示（インデックス登録）されてしまうことがあります。検索結果から完全に除外したい場合は、ページのHTML内に`noindex`メタタグを設定する必要があります。

一言で言うと、`robots.txt`は**「Webサイトの玄関に置かれた、クローラー向けの案内板」**のようなものです。


***
# References